{"pageProps":{"postData":{"id":["2024","03","14","rinha-backend-2024-q1-lmdb"],"contentHtml":"<p>Hoje foi divulgado o\n<a href=\"https://github.com/zanfranceschi/rinha-de-backend-2024-q1/blob/main/RESULTADOS.md\">resultado</a> da\nRinha de Backend - 2024/Q1. Não houve um ranking final dos projetos, mas os logs individuais dos testes de cada projeto\nforam disponibilizados. Que comece a Rinha de Análise de Dados :).</p>\n<p>Resolvi fazer um script que agregasse todos esses dados e gerei\n<a href=\"https://docs.google.com/spreadsheets/d/1weSig7rgM7SGMLGPcOtENnkHZjB7-VrIIkYPPUbWEws/edit#gid=701406682\">essa planilha</a>.</p>\n<p>Me surpreendi com <a href=\"https://github.com/asfernandes/rinhaback24q1-haproxy-mongoose-lmdb\">meu projeto com LMDB</a> ter sido\no único com p75 = 0ms, já que algumas pessoas fizeram sistemas específicos de persistência que poderiam ser muito\nrápidos.</p>\n<p>Meu entendimento, baseado em algumas análises de performance que tinha feito, é que no limite do stress test, isto é,\na otimização final, se trata de chamadas ao kernel (<code>syscalls</code>), que incluia a comunicação TCP/IP e leitura e gravação\ndos dados.</p>\n<p>Quase todos desligaram proteções usadas em sistemas de produção, como <code>fsync</code> de arquivos e logs de transações,\nmas mesmo assim, chamadas síncronas ao kernel ainda eram feitas.</p>\n<p>E aqui entra o detalhe do LMDB. Ele mapeia o arquivo em páginas de memória compartilhadas entre os processos, evitando\n<code>syscalls</code> síncronas para gravações dos dados.</p>\n","layout":"post","title":"Rinha de Backend - 2024/Q1 - LMDB","published":true,"date":"2024-03-14","author":"Adriano dos Santos Fernandes","tags":["c++","docker","web","rinha"]}},"__N_SSG":true}